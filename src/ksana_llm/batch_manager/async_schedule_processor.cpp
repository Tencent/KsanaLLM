/* Copyright 2025 Tencent Inc.  All rights reserved.

==============================================================================*/

#include "ksana_llm/batch_manager/async_schedule_processor.h"
#include "ksana_llm/batch_manager/batch_manager.h"
#include "ksana_llm/profiler/reporter.h"
#include "ksana_llm/utils/logger.h"

namespace ksana_llm {

AsyncScheduleProcessor::~AsyncScheduleProcessor() { Stop(); }

void AsyncScheduleProcessor::Initialize(std::shared_ptr<BatchSchedulerInterface> batch_scheduler,
                                        std::shared_ptr<LlmRuntime> llm_runtime,
                                        std::shared_ptr<MultiBatchController> multi_batch_controller) {
  batch_scheduler_ = batch_scheduler;
  llm_runtime_ = llm_runtime;
  multi_batch_controller_ = multi_batch_controller;
  KLLM_LOG_INFO << "AsyncScheduleProcessor initialized";
}

// å¼‚æ­¥æ¨¡å¼ï¼šä»é˜Ÿåˆ—è·å–å·²å¤„ç†çš„ç»“æœ
ScheduleResult AsyncScheduleProcessor::GetNextScheduleResult(size_t multi_batch_id) {
  // å¦‚æœè¿˜æ²¡æœ‰ä¸ºè¿™ä¸ªmulti_batch_idæäº¤ä»»åŠ¡ï¼Œå…ˆæäº¤ä¸€ä¸ª
  if (pending_results_.size() <= multi_batch_id) {
    pending_results_.resize(multi_batch_id + 1);
  }
  if (!pending_results_[multi_batch_id].valid()) {
    // åˆ›å»ºå¼‚æ­¥ä»»åŠ¡å¹¶æäº¤
    AsyncScheduleTask task(multi_batch_id);
    pending_results_[multi_batch_id] = task.promise.get_future();
    task_queue_.Put(std::move(task));
  }

  // è·å–ç»“æœï¼ˆè¿™ä¼šé˜»å¡ç›´åˆ°ä»»åŠ¡å®Œæˆï¼‰
  ScheduleResult result = pending_results_[multi_batch_id].get();

  // ğŸ”§ å…³é”®ï¼šè·å–è°ƒåº¦ç»“æœåç«‹å³å¤„ç†å¼‚æ­¥è°ƒåº¦çš„fake tokenä¿®æ­£
  // è¿™é‡Œçš„resultåŒ…å«å½“å‰è½®è¦æ‰§è¡Œçš„è°ƒåº¦ï¼Œä½†å…¶ä¸­çš„tokenå¯èƒ½æ˜¯ä¸Šä¸€è½®çš„fake token
  // å¼‚æ­¥è°ƒåº¦å™¨åœ¨å‡†å¤‡å½“å‰è½®ç»“æœæ—¶ï¼Œä¸Šä¸€è½®çš„æ¨ç†å·²ç»å®Œæˆï¼ŒçœŸå®tokenå·²ç»ç”Ÿæˆ
  // æ‰€ä»¥éœ€è¦ç«‹å³ç”¨çœŸå®tokenæ›¿æ¢fake tokenï¼Œç„¶åå†æ‰§è¡Œå½“å‰è½®çš„æ¨ç†
  ProcessAsyncPostProcessing(result);
  // å¦‚æœç»“æœæœ‰æ•ˆï¼Œç«‹å³æäº¤ä¸‹ä¸€è½®ä»»åŠ¡ï¼ˆæµæ°´çº¿å¤„ç†ï¼‰
  if (result.is_valid) {
    AsyncScheduleTask next_task(multi_batch_id);
    pending_results_[multi_batch_id] = next_task.promise.get_future();
    task_queue_.Put(std::move(next_task));
  }

  return result;
}

void AsyncScheduleProcessor::Start() {
  terminated_ = false;

  // å¯åŠ¨å·¥ä½œçº¿ç¨‹ï¼ˆé€šå¸¸1ä¸ªçº¿ç¨‹å°±å¤Ÿäº†ï¼Œå› ä¸ºè°ƒåº¦æœ¬èº«æ˜¯åºåˆ—åŒ–çš„ï¼‰
  worker_threads_.push_back(std::make_unique<std::thread>(&AsyncScheduleProcessor::WorkerLoop, this));

  KLLM_LOG_INFO << "AsyncScheduleProcessor started with " << worker_threads_.size() << " worker threads";
}

void AsyncScheduleProcessor::Stop() {
  if (terminated_) {
    return;
  }

  terminated_ = true;
  task_queue_.Stop();

  for (auto &thread : worker_threads_) {
    if (thread && thread->joinable()) {
      thread->join();
    }
  }
  worker_threads_.clear();

  KLLM_LOG_INFO << "AsyncScheduleProcessor stopped";
}

void AsyncScheduleProcessor::WorkerLoop() {
  while (!terminated_) {
    AsyncScheduleTask task = task_queue_.Get();
    if (terminated_) {
      break;
    }
    ProcessAsyncTask(task);
  }
}

// å¤„ç†å•ä¸ªè°ƒåº¦ä»»åŠ¡ï¼šè°ƒåº¦+æ•°æ®å¤„ç†ï¼ˆè¿™å°±æ˜¯æŠŠåŒæ­¥æ¨¡å¼çš„é€»è¾‘å°è£…æˆä»»åŠ¡ï¼‰
void AsyncScheduleProcessor::ProcessAsyncTask(AsyncScheduleTask &task) {
  PROFILE_EVENT_SCOPE(AsyncScheduleTask, fmt::format("AsyncScheduleTask_{}", task.multi_batch_id));

  ScheduleResult result;
  size_t multi_batch_id = task.multi_batch_id;
  while (!terminated_) {
    // 1. è°ƒç”¨Scheduleï¼ˆå’ŒåŒæ­¥æ¨¡å¼ä¸€æ ·ï¼‰
    std::shared_ptr<ScheduleOutputGroup> schedule_output_group = batch_scheduler_->Schedule(multi_batch_id);

    // 2. åˆå¹¶è°ƒåº¦ç»“æœï¼ˆå’ŒåŒæ­¥æ¨¡å¼ä¸€æ ·ï¼‰
    result.schedule_output = std::make_shared<ScheduleOutput>();
    MergeScheduleOutputGroup(schedule_output_group, *result.schedule_output);

    // 3. æ£€æŸ¥æ˜¯å¦æœ‰è¿è¡Œä¸­çš„è¯·æ±‚ï¼ˆå’ŒåŒæ­¥æ¨¡å¼ä¸€æ ·ï¼‰
    if (schedule_output_group->RunningSize() == 0) {
      // æ²¡æœ‰è¿è¡Œè¯·æ±‚ï¼Œéœ€è¦ç­‰å¾…
      multi_batch_controller_->NotifyCurrentBatchThreadNotReady(multi_batch_id);
      if (batch_scheduler_->IsIdle(multi_batch_id) && !terminated_) {
        batch_scheduler_->WaitUntilHaveReqs(multi_batch_id);
      } else {
        KLLM_LOG_DEBUG << "multi_batch_id=" << multi_batch_id << " not idle, sleep 100ms";
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
      }
      continue;  // ç»§ç»­å¾ªç¯ç­‰å¾…
    }

    // 4. æœ‰è¿è¡Œè¯·æ±‚ï¼Œè¿›è¡Œæ•°æ®å¤„ç†ï¼ˆå’ŒåŒæ­¥æ¨¡å¼ä¸€æ ·ï¼Œä½†éœ€è¦æ·±æ‹·è´ï¼‰
    result.is_valid = true;
    ProcessScheduleDataInternal(result, multi_batch_id);
    break;
  }

  // è¿”å›ç»“æœ
  if (terminated_) {
    result.is_valid = false;
  }
  task.promise.set_value(result);
}

void AsyncScheduleProcessor::ProcessScheduleDataInternal(ScheduleResult &result, size_t multi_batch_id) {
  if (!result.schedule_output || !llm_runtime_) {
    KLLM_LOG_ERROR << "Invalid schedule_output or llm_runtime";
    result.is_valid = false;
    return;
  }

  // è®¾ç½®multi_batch_id
  result.schedule_output->multi_batch_id = multi_batch_id;

  // é‡æ’åºè¯·æ±‚
  llm_runtime_->ReorderInferRequests(result.schedule_output->running_reqs);

  // result.deep_copy_forwarding_tokens = DeepCopyForwardRequest(result.schedule_output->running_reqs);

  // æ„å»ºSamplingRequests
  result.sampling_reqs = std::make_shared<std::vector<SamplingRequest>>();
  llm_runtime_->BuildSamplingRequest(result.schedule_output->multi_batch_id, result.schedule_output->running_reqs,
                                     *result.sampling_reqs);

  for (auto &req : *result.sampling_reqs) {
    DeepCopySamplingRequest(req);
  }

  // è®¡ç®—hidden_token_numï¼ˆç”¨äºåç»­æ¨ç†å¤„ç†ï¼‰
  size_t tokens = 0;
  for (size_t i = 0; i < result.schedule_output->running_reqs.size(); ++i) {
    tokens += result.schedule_output->running_reqs[i]->forwarding_tokens.size() -
              result.schedule_output->running_reqs[i]->kv_cached_token_num;
  }
  result.schedule_output->hidden_token_num = tokens;
}

void AsyncScheduleProcessor::ApplyAsyncForwardingTokens(
    const std::unordered_map<int64_t, std::shared_ptr<std::vector<int>>> &deep_copy_forwarding_tokens,
    std::map<ModelInstance *, std::vector<ForwardRequest *>> &grouped_reqs) {
  for (auto &[model_inst, reqs] : grouped_reqs) {
    for (auto &req : reqs) {
      req->forwarding_tokens = deep_copy_forwarding_tokens.at(req->req_id);
    }
  }
}

// å¼‚æ­¥åå¤„ç†ï¼šä¿®æ­£fake tokenï¼Œåœ¨è·å–è°ƒåº¦ç»“æœåç«‹å³è°ƒç”¨
void AsyncScheduleProcessor::ProcessAsyncPostProcessing(ScheduleResult &result) {
  PROFILE_EVENT_SCOPE(ProcessAsyncPostProcessing, "ProcessAsyncPostProcessing");
  if (!result.is_valid) {
    return;
  }
  KLLM_LOG_DEBUG << "ProcessAsyncPostProcessing: processing " << result.schedule_output->running_reqs.size()
                 << " requests";
  batch_scheduler_->NotifyAsyncRecomputedRequests();
  for (auto &req : result.schedule_output->running_reqs) {
    // åªæœ‰åœ¨decodeé˜¶æ®µæ‰éœ€è¦ä¿®æ­£fake tokenï¼Œprefillé˜¶æ®µä¸éœ€è¦ä¿®æ­£
    // å› ä¸ºprefillé˜¶æ®µå¤„ç†çš„æ˜¯è¾“å…¥tokenåºåˆ—ï¼Œä¸æ¶‰åŠç”Ÿæˆçš„fake token
    // å¦‚æœè¿™ä¸€è½®æ˜¯prefill ä¹Ÿæœ‰å¯èƒ½éœ€è¦ä¿®æ­£ï¼Œå› ä¸ºæœ‰generate token
    // åœ¨ä»€ä¹ˆæ—¶å€™éœ€è¦ä¿®æ­£å‘¢ï¼Œå…¶å®åº”è¯¥çœ‹ä¸‹ä¸€è½®ï¼Œå¦‚æœæ˜¯åšdecodeï¼Œé‚£è¿™ä¸€è½®åº”è¯¥ä¿®æ­£ï¼Œå¦‚æœåšprefillå°±ä¸éœ€è¦ä¿®æ­£ã€‚
    // æ–°çš„è¯·æ±‚åˆšåˆšåŠ å…¥runningé˜Ÿåˆ—ï¼Œstep = 0ï¼Œstep > 0è¯´æ˜ç”Ÿæˆäº†è¯·æ±‚ï¼Œéœ€è¦ä¿®æ­£ã€‚
    if (req->step > 0) {
      // æ›´æ–°forwarding_tokensä¸­çš„fake tokenä¸ºçœŸå®token
      std::vector<int> draft_tokens = req->draft_tokens.GetDraftTokens();
      req->forwarding_tokens.resize(req->forwarding_tokens.size() - req->forwarding_tokens_draft_num +
                                    req->accepted_tokens.size() - kStepGenerateTokenNum - req->last_step_draft_num);
      // æ›´æ–°kv cacheç›¸å…³ä¿¡æ¯
      req->kv_cached_token_num = req->forwarding_tokens.size();
      req->prefix_cache_len = req->kv_cached_token_num;
      req->cache_manager->UpdateRequestTokens(req->req_id, req->forwarding_tokens, req->kv_cached_token_num,
                                              req->kv_cache_blocks);
      // æ·»åŠ å½“å‰ç”Ÿæˆçš„çœŸå®token
      req->forwarding_tokens.emplace_back(req->generated_token);
      req->last_step_token_num = req->accepted_tokens.size() + kStepGenerateTokenNum;
      req->last_step_draft_num = draft_tokens.size();

      req->output_mutex.lock();
      req->output_tokens.insert(req->output_tokens.end(),
                                req->forwarding_tokens.end() - req->accepted_tokens.size() - kStepGenerateTokenNum,
                                req->forwarding_tokens.end());
      req->output_mutex.unlock();
      req->forwarding_tokens.insert(req->forwarding_tokens.end(), draft_tokens.begin(), draft_tokens.end());
      req->forwarding_tokens_draft_num = req->draft_tokens.size();
      // è®¾ç½®é‡‡æ ·tokenæ•°é‡
      req->sampling_token_num =
          req->logits_custom_length > 0 ? req->logits_custom_length : req->draft_tokens.size() + kStepGenerateTokenNum;
    }
  }

  llm_runtime_->ReorderInferRequests(result.schedule_output->running_reqs);

  auto deep_copy_forwarding_tokens = DeepCopyForwardRequest(result.schedule_output->running_reqs);
  // æ„å»ºForwardRequests
  result.grouped_reqs = std::make_shared<std::map<ModelInstance *, std::vector<ForwardRequest *>>>();
  llm_runtime_->BuildForwardRequests(result.schedule_output->multi_batch_id, result.schedule_output->running_reqs,
                                     *result.grouped_reqs);

  ApplyAsyncForwardingTokens(*deep_copy_forwarding_tokens, *result.grouped_reqs);

  // å¤„ç†sampling_reqsä¸­çš„SamplingRequest - ä½¿ç”¨origin_tokensè·å–çœŸå®token
  llm_runtime_->DeepCopyAndSyncSamplingRequests(result.schedule_output->running_reqs, *result.sampling_reqs);
}

}  // namespace ksana_llm