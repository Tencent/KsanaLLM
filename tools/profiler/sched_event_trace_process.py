#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script to parse and process sched_event_tracer.cpp CSV output.
This script reads the CSV file generated by sched_event_tracer.cpp,
parses the data, sorts it by the time_ns field, and converts it to
Chrome Trace Event format JSON.
"""
import argparse
import csv
import glob
import json
import os
import sys
from typing import List, Dict, Any


def parse_csv(csv_file_path: str) -> List[Dict[str, Any]]:
    """
    Parse the CSV file generated by sched_event_tracer.cpp.

    Args:
        csv_file_path: Path to the CSV file

    Returns:
        List of dictionaries, each representing a row in the CSV file
    """
    if not os.path.exists(csv_file_path):
        print(f"Error: File {csv_file_path} does not exist.")
        return []

    events = []

    with open(csv_file_path, "r") as file:
        reader = csv.DictReader(file)

        for row in reader:
            # Convert numeric fields to appropriate types
            if "node_rank" in row:
                row["node_rank"] = int(row["node_rank"])
            else:
                row["node_rank"] = 0  # Default value if not present

            row["req_id"] = int(row["req_id"])
            row["time_ns"] = int(row["time_ns"])
            row["rank"] = int(row["rank"])
            row["multi_batch_id"] = int(row["multi_batch_id"])
            row["attn_dp_group_id"] = int(row["attn_dp_group_id"])

            # These fields might be empty for some events
            if row["forwarding_token_num"]:
                row["forwarding_token_num"] = int(row["forwarding_token_num"])
            else:
                row["forwarding_token_num"] = 0

            if row["seq_len"]:
                row["seq_len"] = int(row["seq_len"])
            else:
                row["seq_len"] = 0

            events.append(row)

    return events


def parse_multiple_csv_files(file_patterns: List[str]) -> List[Dict[str, Any]]:
    """
    Parse multiple CSV files using glob patterns.

    Args:
        file_patterns: List of file paths or glob patterns

    Returns:
        Combined list of dictionaries from all CSV files
    """
    all_events = []
    processed_files = 0

    for pattern in file_patterns:
        # Expand glob pattern to get all matching files
        matching_files = glob.glob(pattern)

        if not matching_files:
            print(f"Warning: No files match pattern '{pattern}'")
            continue

        for file_path in matching_files:
            events = parse_csv(file_path)
            if events:
                all_events.extend(events)
                processed_files += 1
                print(f"Processed {file_path}: {len(events)} events")

    if processed_files == 0:
        print("Error: No valid CSV files were processed.")
        sys.exit(1)

    print(f"Total: {len(all_events)} events from {processed_files} files")
    return all_events


def sort_events_by_time(events: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Sort events by the time_ns field.

    Args:
        events: List of event dictionaries

    Returns:
        Sorted list of event dictionaries
    """
    return sorted(events, key=lambda x: x["time_ns"])


def get_event_color(event_type: str) -> str:
    """
    Get the color name for an event type based on RequestEventType in sched_event_tracer.h.

    Args:
        event_type: The type of event

    Returns:
        Chrome trace viewer color name
    """
    # Define color mapping for different event types using valid Chrome trace viewer colors
    # Valid Chrome trace viewer colors include:
    # "good" (green), "bad" (red), "terrible" (dark red), "yellow", "olive",
    # "rail_response" (blue), "rail_animation" (light green), "startup" (dark blue),
    # "heap_dump_stack_frame" (teal), "heap_dump_object_type" (purple),
    # "thread_state_iowait" (pink/red), "thread_state_running" (light grey),
    # "thread_state_runnable" (light green/yellow)

    # ForwardingLayers should be pink, others should use lighter colors
    # Valid colors are defined in
    # https://github.com/catapult-project/catapult/blob/master/tracing/tracing/base/color_scheme.html
    color_map = {
        # ForwardingLayers in pink (using thread_state_iowait which is pinkish-red)
        "ForwardingLayers": "thread_state_iowait",  # Pink/red color for ForwardingLayers
        "StreamSynchronize": "heap_dump_child_node_arrow",
        # Other event types with lighter colors
        "SchedBegin": "yellow",  # Yellow (light)
        "DropReq": "bad",  # Red (lighter than terrible)
        "FinishReq": "good",  # Green (light)
        "SwapOut": "rail_response",  # Blue (light)
        "SwapIn": "rail_animation",  # Light green
        "Recompute": "heap_dump_object_type",  # Purple (light)
        "Schedule": "yellow",  # Yellow (light)
        "PrepareForwarding": "olive",  # Olive (light)
        "EmbLookup": "heap_dump_stack_frame",  # Teal (light)
        "LmHead": "thread_state_runnable",  # Light green/yellow
        "RecvHiddenUnitBuffer": "grey",
    }

    return color_map.get(
        event_type, "thread_state_running"
    )  # Default to light grey if not found


def convert_to_chrome_trace_format(
    events: List[Dict[str, Any]],
    excluded_event_types: List[str] = None,
    node_first: bool = False,
) -> Dict[str, Any]:
    """
    Convert events to Chrome Trace Event format.

    Args:
        events: List of event dictionaries
        excluded_event_types: List of event types to exclude from the output

    Returns:
        Dictionary in Chrome Trace Event format
    """
    if excluded_event_types is None:
        excluded_event_types = ["SchedBegin"]  # Default to excluding SchedBegin

    trace_events = []

    for event in events:
        # Skip excluded event types
        if event["event_type"] in excluded_event_types:
            continue

        # Map phase from 'Begin'/'End' to 'B'/'E'
        phase = "B" if event["phase"] == "Begin" else "E"

        # Get color for this event type
        color_name = get_event_color(event["event_type"])
        if event["req_id"] != -1:  # Only add for Begin events to avoid duplicates
            trace_event = {
                "name": event["event_type"],
                "cat": "PERF",
                "ph": phase,
                "pid": event["req_id"],
                "tid": event["node_rank"],
                "ts": event["time_ns"] / 1000,
                "cname": color_name,  # Add color name
            }

            trace_events.append(trace_event)
        else:
            # Try to record metrics
            record_metrics = False
            if event["node_rank"] == 0 and (
                (event["event_type"] == "Schedule" and phase == "B")
                or (event["event_type"] == "LmHead" and phase == "E")
            ):
                record_metrics = True

            if record_metrics:
                # Add event for req num
                token_event = {
                    "name": f"{event['req_num']}",
                    "cat": "INFO",
                    "ph": phase,
                    "pid": "metrics",
                    "tid": f"req_num batch {event['multi_batch_id']}",
                    "ts": event["time_ns"] / 1000,
                    "cname": "thread_state_sleeping",
                }
                trace_events.append(token_event)

                # Add event for forwarding token number
                token_event = {
                    "name": f"{event['forwarding_token_num']}",
                    "cat": "INFO",
                    "ph": phase,
                    "pid": "metrics",
                    "tid": f"forwarding_token_num batch {event['multi_batch_id']}",
                    "ts": event["time_ns"] / 1000,
                    "cname": "thread_state_runnable",
                }
                trace_events.append(token_event)

                # Add event for forwarding token number
                token_event = {
                    "name": f"{event['seq_len']}",
                    "cat": "INFO",
                    "ph": phase,
                    "pid": "metrics",
                    "tid": f"seq_len batch {event['multi_batch_id']}",
                    "ts": event["time_ns"] / 1000,
                    "cname": "grey",
                }
                trace_events.append(token_event)

            # Add event record
            if node_first:
                pid = f"Node {event['node_rank']}"
                tid = f"batch {event['multi_batch_id']}"
            else:
                pid = f"batch {event['multi_batch_id']}"
                tid = f"Node {event['node_rank']}"

            trace_event = {
                "name": event["event_type"],
                "cat": "PERF",
                "ph": phase,
                "pid": pid,
                "tid": tid,
                "ts": event["time_ns"] / 1000,
                "cname": color_name,  # Add color name
            }

            trace_events.append(trace_event)

    return {"traceEvents": trace_events, "displayTimeUnit": "ns"}


def write_json(data: Dict[str, Any], output_file_path: str) -> None:
    """
    Write data to a JSON file.

    Args:
        data: Dictionary to write as JSON
        output_file_path: Path to the output JSON file
    """
    with open(output_file_path, "w") as file:
        json.dump(data, file, indent=2)

    print(f"JSON data written to {output_file_path}")


def analyze_events(events: List[Dict[str, Any]]) -> None:
    """
    Perform basic analysis on the events.

    Args:
        events: List of event dictionaries
    """
    if not events:
        print("No events to analyze.")
        return

    # Count events by type
    event_types = {}
    for event in events:
        event_type = event["event_type"]
        if event_type not in event_types:
            event_types[event_type] = 0
        event_types[event_type] += 1

    print("\nEvent Type Distribution:")
    for event_type, count in event_types.items():
        print(f"  {event_type}: {count}")

    # Calculate time range
    min_time = min(events, key=lambda x: x["time_ns"])["time_ns"]
    max_time = max(events, key=lambda x: x["time_ns"])["time_ns"]
    duration_ms = (max_time - min_time) / 1_000_000  # Convert ns to ms

    print(f"\nTime Range: {duration_ms:.2f} ms")
    print(f"Total Events: {len(events)}")

    # Count events by phase
    begin_events = sum(1 for event in events if event["phase"] == "Begin")
    end_events = sum(1 for event in events if event["phase"] == "End")

    print(f"Begin Events: {begin_events}")
    print(f"End Events: {end_events}")


def main():
    parser = argparse.ArgumentParser(
        description="Process sched_event_tracer.cpp CSV output"
    )
    parser.add_argument(
        "input_files",
        nargs="+",
        help="Paths to input CSV files (supports wildcards like *.csv)",
    )
    parser.add_argument(
        "-o",
        "--output",
        help="Path to the output JSON file (default: trace_events.json)",
    )
    parser.add_argument(
        "-a",
        "--analyze",
        action="store_true",
        help="Perform basic analysis on the events",
    )
    parser.add_argument(
        "-e",
        "--exclude",
        nargs="+",
        default=["SchedBegin"],
        help="Event types to exclude from JSON output (default: SchedBegin)",
    )
    parser.add_argument(
        "--include-all",
        action="store_true",
        help="Include all event types (overrides --exclude)",
    )
    parser.add_argument(
        "--node-first",
        action="store_true",
        help="Generate trace with node as process, otherwise batch as process",
    )

    args = parser.parse_args()

    # Set default output file if not specified
    if not args.output:
        args.output = "trace_events.json"

    # Parse multiple CSV files and combine events
    events = parse_multiple_csv_files(args.input_files)

    # Sort all events by time
    sorted_events = sort_events_by_time(events)

    # Convert to Chrome Trace Event format and write to JSON file
    excluded_event_types = [] if args.include_all else args.exclude
    trace_data = convert_to_chrome_trace_format(
        sorted_events, excluded_event_types, args.node_first
    )
    write_json(trace_data, args.output)

    # Perform analysis if requested
    if args.analyze:
        analyze_events(sorted_events)


if __name__ == "__main__":
    main()
